{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "hw_lesson_4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-obTobHkgrk9",
        "colab_type": "text"
      },
      "source": [
        "# Тема “Классификация текста”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTwqwXGwgrk-",
        "colab_type": "text"
      },
      "source": [
        "В качестве заготовки для задания прогоним часть 2ого домашнего задания. Нам необходимо получить разреженные матрицы, используя CountVectorizer, TfidfVectorizer для 'tweet_stemmed' и 'tweet_lemmatized' столбцов (4 матрицы).\n",
        "\n",
        "**Задание 1.**\n",
        "<br>Построим модель LogisticRegression, используя Bag-of-Words признаки для столбца combine_df['tweet_stemmed']. \n",
        "- Поделим Bag-of-Words признаки на train, test (train заканчивается на 31962 строке combine_df)\n",
        "- Ответами является столбец train_df['label']\n",
        "- Рассчитаем predict_proba, приведем prediction в в бинарный вид: если предсказание >= 0.3 то 1, иначе 0, тип заменим на int\n",
        "- Рассчитаем f1_score \n",
        "\n",
        "Повторим аналогично для столбца combine_df['tweet_lemmatized'].\n",
        "\n",
        "**Задание 2.**\n",
        "<br>Построим модель LogisticRegression, используя TF-IDF признаки для столбца combine_df['tweet_stemmed']. \n",
        "- Поделим TF-IDF признаки на train, test (train заканчивается на 31962 строке combine_df)\n",
        "- Ответами является столбец train_df['label']\n",
        "- Рассчитаем predict_proba, приведем prediction в в бинарный вид: если предсказание >= 0.3 то 1, иначе 0, тип заменим на int\n",
        "- Рассчитаем f1_score \n",
        "\n",
        "Повторим аналогично для столбца combine_df['tweet_lemmatized'].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoBTkVPOgrk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\", category=Warning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDrPpHPEgrlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('combine_df.pickle', 'rb') as f:\n",
        "    combine_df = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhscCfrcgrlE",
        "colab_type": "code",
        "colab": {},
        "outputId": "73254696-3f8c-479b-80f4-2a62562cc0b6"
      },
      "source": [
        "combine_df = combine_df[0:3196]\n",
        "\n",
        "for col in ['tweet_stemmed', 'tweet_lemmatized']:\n",
        "    combine_df[col] = combine_df[col].str.join(' ')\n",
        "\n",
        "combine_df.head(n=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>when father is dysfunctional and is so selfish...</td>\n",
              "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
              "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
              "      <td>father dysfunct selfish drag kid dysfunct run</td>\n",
              "      <td>father dysfunctional selfish drag kid dysfunct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
              "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
              "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
              "      <td>thank lyft credit use caus offer wheelchair va...</td>\n",
              "      <td>thank lyft credit use cause offer wheelchair v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>[bihday, your, majesty]</td>\n",
              "      <td>[bihday, majesty]</td>\n",
              "      <td>bihday majesti</td>\n",
              "      <td>bihday majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>model love you take with you all the time in ur</td>\n",
              "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
              "      <td>[model, love, take, time, ur]</td>\n",
              "      <td>model love take time ur</td>\n",
              "      <td>model love take time ur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>factsguide society now motivation</td>\n",
              "      <td>[factsguide, society, now, motivation]</td>\n",
              "      <td>[factsguide, society, motivation]</td>\n",
              "      <td>factsguid societi motiv</td>\n",
              "      <td>factsguide society motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet  \\\n",
              "0   1    0.0  when father is dysfunctional and is so selfish...   \n",
              "1   2    0.0  thanks for lyft credit cannot use cause they d...   \n",
              "2   3    0.0                                bihday your majesty   \n",
              "3   4    0.0    model love you take with you all the time in ur   \n",
              "4   5    0.0                  factsguide society now motivation   \n",
              "\n",
              "                                         tweet_token  \\\n",
              "0  [when, father, is, dysfunctional, and, is, so,...   \n",
              "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
              "2                            [bihday, your, majesty]   \n",
              "3  [model, love, you, take, with, you, all, the, ...   \n",
              "4             [factsguide, society, now, motivation]   \n",
              "\n",
              "                                tweet_token_filtered  \\\n",
              "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
              "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
              "2                                  [bihday, majesty]   \n",
              "3                      [model, love, take, time, ur]   \n",
              "4                  [factsguide, society, motivation]   \n",
              "\n",
              "                                       tweet_stemmed  \\\n",
              "0      father dysfunct selfish drag kid dysfunct run   \n",
              "1  thank lyft credit use caus offer wheelchair va...   \n",
              "2                                     bihday majesti   \n",
              "3                            model love take time ur   \n",
              "4                            factsguid societi motiv   \n",
              "\n",
              "                                    tweet_lemmatized  \n",
              "0  father dysfunctional selfish drag kid dysfunct...  \n",
              "1  thank lyft credit use cause offer wheelchair v...  \n",
              "2                                     bihday majesty  \n",
              "3                            model love take time ur  \n",
              "4                      factsguide society motivation  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyb6ZckdgrlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = combine_df[['tweet_stemmed', 'tweet_lemmatized']]\n",
        "y = combine_df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjj7r2klgrlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkOJWa_RgrlL",
        "colab_type": "text"
      },
      "source": [
        "Векторайзеры и логистическая регрессия (с L1-регуляризацией и кросс-валидацией):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLD7WrrKgrlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "\n",
        "clf = LogisticRegressionCV(\n",
        "    cv=3,\n",
        "    penalty='l1',\n",
        "    scoring='f1',\n",
        "    solver='saga',\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "181a8SJzgrlO",
        "colab_type": "text"
      },
      "source": [
        "Функция для векторизации обучающей и тестовой выборок, обучения логистической регрессии и получения f1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp7x4SjpgrlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_eval_pipeline(X_train, y_train, X_test, y_test, vectorizer, clf):\n",
        "    \n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "    \n",
        "    clf.fit(X_train_vectorized, y_train)\n",
        "    \n",
        "    y_proba = clf.predict_proba(X_test_vectorized)\n",
        "    y_proba = y_proba[:, 1]\n",
        "    \n",
        "    f1_value = f1_score(y_test, y_proba >= 0.3)\n",
        "    \n",
        "    return f1_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVUzB6FsgrlQ",
        "colab_type": "text"
      },
      "source": [
        "f1 для каждого из типов нормализации и векторизации слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CktLCskqgrlQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "ffb3f25b-ae25-4dd9-d06c-c833ec440faa"
      },
      "source": [
        "for n_type, col in zip(['Stemming', 'Lemmatization'],\n",
        "                       ['tweet_stemmed', 'tweet_lemmatized']):\n",
        "    print(n_type)\n",
        "    for v_type, vectorizer in zip(['Bag-of-Words', 'TF-IDF'],\n",
        "                                  [count_vectorizer, tfidf_vectorizer]):\n",
        "        f1_value = train_and_eval_pipeline(X_train[col], y_train,\n",
        "                                           X_test[col], y_test,\n",
        "                                           vectorizer, clf)\n",
        "        print(f' {v_type}: {f1_value:.6f}')\n",
        "    print(end='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming\n",
            " Bag-of-Words: 0.452174\n",
            " TF-IDF: 0.458716\n",
            "\n",
            "Lemmatization\n",
            " Bag-of-Words: 0.464286\n",
            " TF-IDF: 0.477064\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}